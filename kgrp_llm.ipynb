{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification\n",
    "\n",
    "# Prevents many tokenizer warnings\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"Utility functions called by the model operations .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_stamp):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.time_stamp = time_stamp\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        return self.time_stamp\n",
    "\n",
    "    def write_log(self, s, path = 'log.out', prnt = True):\n",
    "        ''\n",
    "        \n",
    "        f = open(self.time_stamp + path , \"a\")\n",
    "        f.write('\\n' + s)\n",
    "        if prnt:\n",
    "            print(s)\n",
    "        f.close()\n",
    "\n",
    "    def load_relations(self, path: str):\n",
    "        ''\n",
    "\n",
    "        relations = []\n",
    "        with open(path) as f:\n",
    "            for line in f.readlines():\n",
    "                relations.append(line.strip())\n",
    "        \n",
    "        print('\\nRelations loaded')\n",
    "        return relations\n",
    "\n",
    "    # Reproduce\n",
    "    def seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args,\n",
    "                 triples_filename: str,\n",
    "                 relations: list,\n",
    "                ) -> None:\n",
    "        \"\"\"This constructor loads the necessary data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.relations = relations\n",
    "        self.cache = {}\n",
    "\n",
    "        # Read file\n",
    "        f = open( os.path.join(args.dataset_directory, triples_filename) )\n",
    "        file_lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        # Load lines based on the specified data amount\n",
    "        self.lines = file_lines[: int(len(file_lines) * args.data_size) ]\n",
    "        print(f'\\n{len(self.lines)} triples loaded')\n",
    "\n",
    "        # Tokenizer\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(args.model_id,\n",
    "                                                   token = self.args.repo_token,)\n",
    "        tokenizer.padding_side = 'right'\n",
    "        tokenizer.truncation_side = 'right'\n",
    "        tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        tokenizer.model_max_length = args.padding\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Load entity translations\n",
    "        self.entities_dict = {}\n",
    "        with open( os.path.join(args.dataset_directory, args.entities_filename) ) as f:\n",
    "            for line in f.readlines():\n",
    "                fields = line.split('\\t')\n",
    "                fields = [p.strip() for p in fields]\n",
    "                self.entities_dict[ fields[0] ] = fields[1]\n",
    "        \n",
    "        print(f'\\n{len(self.entities_dict.keys())} entity translations loaded')\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Check the cache\n",
    "        if index in self.cache.keys():\n",
    "            return self.cache[index]\n",
    "\n",
    "        # Create triple from a dataset line\n",
    "\n",
    "        fields = self.lines[index].split('\\t')\n",
    "        fields = [p.strip() for p in fields]\n",
    "\n",
    "        # Prepare Y label\n",
    "        rel = fields[1]\n",
    "        rel_index = self.relations.index(rel)\n",
    "        relations_tagged = [0.0] * len(self.relations)\n",
    "        relations_tagged[ rel_index ] = 1.0\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer([[self.entities_dict[fields[0]],\\\n",
    "                                   self.entities_dict[fields[2]]]],\n",
    "                  padding='max_length',\n",
    "                  truncation = True,\n",
    "                  return_attention_mask=True,\n",
    "                  return_tensors=\"pt\")\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(0).squeeze(0)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0).squeeze(0)\n",
    "\n",
    "        result = (inputs, torch.tensor(relations_tagged))\n",
    "\n",
    "        self.cache[index] = result\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,relations, model_id, repo_token) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.relations = relations\n",
    "        self.model_id = model_id\n",
    "        self.repo_token = repo_token\n",
    "        \n",
    "    \n",
    "    def get_model(self):\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            self.model_id,\n",
    "            token = self.repo_token,\n",
    "            num_labels= len(self.relations))\n",
    "        model.config.pad_token_id = 4\n",
    "        for param in model.parameters():\n",
    "            if param.dtype == torch.float32 or \\\n",
    "            param.dtype == torch.float16 :\n",
    "                param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(relations,\n",
    "    repo_token,\n",
    "    args,\n",
    "    utils,\n",
    "    generator,\n",
    "    training_triples,\n",
    "    validation_triples,\n",
    "    device\n",
    "    ):\n",
    "    \"\"\"Train\n",
    "    \"\"\"\n",
    "\n",
    "    # Load training set\n",
    "    training_set = KGDataset(args=args,\n",
    "                             relations=relations,\n",
    "                             triples_filename=training_triples)\n",
    "    training_generator = DataLoader(training_set,\n",
    "                                    batch_size = args.batch_size,\n",
    "                                    worker_init_fn=utils.seed_worker,\n",
    "                                    generator=generator,)\n",
    "    validation_set = KGDataset(args=args,\n",
    "                               relations=relations,\n",
    "                               triples_filename=validation_triples)\n",
    "    validation_generator = DataLoader(validation_set,\n",
    "                                      batch_size = args.batch_size,\n",
    "                                    worker_init_fn=utils.seed_worker,\n",
    "                                    generator=generator,)\n",
    "\n",
    "    # Initializing the model\n",
    "    llama = Llama(relations, args.model_id, repo_token)\n",
    "    model = llama.get_model()\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate )\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, gamma=args.decay, step_size = 1)\n",
    "    model.to(device)\n",
    "\n",
    "    v_loss = 1_000_000\n",
    "    no_change_counter = 1\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        model.train()\n",
    "        loop = tqdm(training_generator, disable = not args.verbose)\n",
    "\n",
    "        # Loop over batches in an epoch using DataLoader\n",
    "        for _, data in enumerate(loop):\n",
    "            inputs = data[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(**inputs).logits\n",
    "            loss = loss_f(logits , data[1].to(device))\n",
    "            loss.backward()       \n",
    "            optimizer.step()\n",
    "            last_loss = loss.item()\n",
    "        \n",
    "        v_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(validation_generator):\n",
    "                inputs = data[0].to(device)\n",
    "                logits = model(**inputs).logits\n",
    "                loss = loss_f(logits , data[1].to(device))\n",
    "                v_losses.append(loss)\n",
    "            \n",
    "            v_loss_epoch = sum(v_losses) / len(v_losses)\n",
    "            utils.write_log(f'lr {lr:8f} train loss {last_loss:.8f} val loss {v_loss_epoch:.8f}')\n",
    "\n",
    "            if v_loss_epoch < v_loss:\n",
    "                v_loss = v_loss_epoch\n",
    "                no_change_counter = 0\n",
    "                torch.save(model.state_dict(), utils.get_timestamp()+'chkpnt.pt')\n",
    "            elif no_change_counter > args.patience - 1:\n",
    "                break\n",
    "            else:\n",
    "                no_change_counter += 1\n",
    "        \n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be removed from online repo\n",
    "class Args(argparse.Namespace):\n",
    "    pass\n",
    "\n",
    "args=Args()\n",
    "args.model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "args.data_size = 0.01\n",
    "args.dataset_directory = 'data/FB15K'\n",
    "args.entities_filename = 'entity2text.txt'\n",
    "args.padding = 40\n",
    "args.batch_size = 16\n",
    "args.patience = 5\n",
    "args.repo_token = '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'\n",
    "args.task = 'train'\n",
    "args.learning_rate = 2e-5\n",
    "args.decay = 0.35\n",
    "args.epochs = 3\n",
    "args.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_size', required = True, type = float)\n",
    "    parser.add_argument('--dataset_directory', required = True, type = str)\n",
    "    parser.add_argument('--entities_filename', required = True, type = str)\n",
    "    parser.add_argument('--model_id', required = True, type = str)\n",
    "    parser.add_argument('--repo_token', required = True, type = str)\n",
    "    parser.add_argument('--padding', required = True, type = int)\n",
    "    parser.add_argument('--learning_rate', required = True, type = float)\n",
    "    parser.add_argument('--decay', required = True, type = float)\n",
    "    parser.add_argument('--task', required = True, type = str)\n",
    "    parser.add_argument('--batch_size', required = True, type = int)\n",
    "    parser.add_argument('--patience', required = True, type = int)\n",
    "    parser.add_argument('--epochs', required = True, type = int)\n",
    "    parser.add_argument('--verbose', required = True)\n",
    "    \n",
    "    # Keep for online repo\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # Random seeds\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    # Initializations\n",
    "    \n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')+'.out'\n",
    "\n",
    "    # To be removed from the online repo\n",
    "\n",
    "    # Load utilities\n",
    "    utils = Utils(time_stamp)\n",
    "    relations = utils.load_relations(args.dataset_directory + '/relations.txt')    \n",
    "\n",
    "    # Loading GPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    utils.write_log('\\ndevice ' + str(device))\n",
    "\n",
    "    if args.task == 'train':\n",
    "        train(relations=relations, args=args, repo_token = args.repo_token,\\\n",
    "              utils = utils, generator=g,\n",
    "              training_triples='train.tsv',\n",
    "              validation_triples='dev.tsv',\n",
    "              device=device)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
