{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification\n",
    "from transformers import LlamaModel, LlamaConfig\n",
    "\n",
    "# Prevents many tokenizer warnings\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"Utility functions called by the model operations .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, time_stamp):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.time_stamp = time_stamp\n",
    "        self.write_log(f'\\Time stamp {time_stamp}')\n",
    "    \n",
    "    def get_timestamp(self):\n",
    "        return self.time_stamp\n",
    "\n",
    "    def write_log(self, s, path = 'log.out', prnt = True):\n",
    "        ''\n",
    "        \n",
    "        f = open(self.time_stamp + path , \"a\")\n",
    "        f.write('\\n' + s)\n",
    "        if prnt:\n",
    "            print(s)\n",
    "        f.close()\n",
    "\n",
    "    def load_relations(self, path: str):\n",
    "        ''\n",
    "\n",
    "        relations = []\n",
    "        with open(path) as f:\n",
    "            for line in f.readlines():\n",
    "                relations.append(line.strip())\n",
    "        \n",
    "        print('\\nRelations loaded')\n",
    "        return relations\n",
    "\n",
    "    # Reproduce\n",
    "    def seed_worker(self, worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args,\n",
    "                 triples_filename: str,\n",
    "                 relations: list,\n",
    "                 is_training= False,\n",
    "                ) -> None:\n",
    "        \"\"\"This constructor loads the necessary data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.args = args\n",
    "        self.relations = relations\n",
    "        self.cache = {}\n",
    "\n",
    "        # Read file\n",
    "        f = open( os.path.join(args.dataset_directory, triples_filename) )\n",
    "        file_lines = f.readlines()\n",
    "        f.close()\n",
    "\n",
    "        # Load lines based on the specified data amount\n",
    "        if is_training:\n",
    "            self.lines = file_lines[: int(len(file_lines) * args.data_size) ]\n",
    "        else:\n",
    "            self.lines = file_lines\n",
    "        print(f'\\n{len(self.lines)} {triples_filename} triples loaded')\n",
    "\n",
    "        # Tokenizer\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(args.model_id,\n",
    "                                                   token = self.args.repo_token,)\n",
    "        tokenizer.padding_side = 'right'\n",
    "        tokenizer.truncation_side = 'right'\n",
    "        tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        tokenizer.model_max_length = args.padding\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Load entity translations\n",
    "        self.entities_dict = {}\n",
    "        with open( os.path.join(args.dataset_directory, args.entities_filename) ) as f:\n",
    "            for line in f.readlines():\n",
    "                fields = line.split('\\t')\n",
    "                fields = [p.strip() for p in fields]\n",
    "                self.entities_dict[ fields[0] ] = fields[1]\n",
    "        \n",
    "        print(f'\\n{len(self.entities_dict.keys())} entity translations loaded')\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.lines)\n",
    "    \n",
    "    def gettext(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        fields = self.lines[index].split('\\t')\n",
    "        fields = [p.strip() for p in fields]\n",
    "        head = self.entities_dict[fields[0]]\n",
    "        tail = self.entities_dict[fields[2]]\n",
    "        rel = fields[1]\n",
    "        return head, rel, tail, fields[0],fields[2]\n",
    "    \n",
    "    def tokenizer_len(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        # Check the cache\n",
    "        if index in self.cache.keys():\n",
    "            return self.cache[index]\n",
    "\n",
    "        # Create triple from a dataset line\n",
    "\n",
    "        fields = self.lines[index].split('\\t')\n",
    "        fields = [p.strip() for p in fields]\n",
    "\n",
    "        # Prepare Y label\n",
    "        rel = fields[1]\n",
    "        rel_index = self.relations.index(rel)\n",
    "        relations_tagged = [0.0] * len(self.relations)\n",
    "        relations_tagged[ rel_index ] = 1.0\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer([[self.entities_dict[fields[0]],\\\n",
    "                                   self.entities_dict[fields[2]]]],\n",
    "                  padding='max_length',\n",
    "                  truncation = True,\n",
    "                  return_attention_mask=True,\n",
    "                  return_tensors=\"pt\")\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(0).squeeze(0)\n",
    "        inputs['attention_mask'] = inputs['attention_mask'].squeeze(0).squeeze(0)\n",
    "\n",
    "        result = (inputs, torch.tensor(relations_tagged))\n",
    "\n",
    "        self.cache[index] = result\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,relations, model_id, repo_token, attention_dropout) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.relations = relations\n",
    "        self.model_id = model_id\n",
    "        self.repo_token = repo_token\n",
    "        self.attention_dropout = attention_dropout\n",
    "        \n",
    "    \n",
    "    def get_model(self):\n",
    "        configuration = LlamaConfig(attention_dropout = self.attention_dropout)\n",
    "        configuration.num_labels = len(self.relations)\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            self.model_id,\n",
    "            config = configuration,\n",
    "            token = self.repo_token,\n",
    "            \n",
    "            )\n",
    "        model.config.pad_token_id = 4\n",
    "        for param in model.parameters():\n",
    "            if param.dtype == torch.float32 or \\\n",
    "            param.dtype == torch.float16 :\n",
    "                param.data = param.data.to(torch.bfloat16)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(relations,\n",
    "    args,\n",
    "    utils,\n",
    "    generator,\n",
    "    training_triples,\n",
    "    validation_triples,\n",
    "    device\n",
    "    ):\n",
    "    \"\"\"Train\n",
    "    \"\"\"\n",
    "\n",
    "    print(args)\n",
    "\n",
    "    # Load training set\n",
    "    training_set = KGDataset(args=args,\n",
    "                             relations=relations,\n",
    "                             triples_filename=training_triples,\n",
    "                             is_training=True)\n",
    "    training_generator = DataLoader(training_set,\n",
    "                                    batch_size = args.batch_size,\n",
    "                                    worker_init_fn=utils.seed_worker,\n",
    "                                    generator=generator,)\n",
    "    validation_set = KGDataset(args=args,\n",
    "                               relations=relations,\n",
    "                               triples_filename=validation_triples)\n",
    "    validation_generator = DataLoader(validation_set,\n",
    "                                      batch_size = args.batch_size,\n",
    "                                    worker_init_fn=utils.seed_worker,\n",
    "                                    generator=generator,)\n",
    "\n",
    "    # Initializing the model\n",
    "    llama = Llama(relations, args.model_id, args.repo_token, args.attention_dropout)\n",
    "    model = llama.get_model()\n",
    "    model.resize_token_embeddings(training_set.tokenizer_len())\n",
    "    model.to(device)\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate )\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, gamma=args.decay, step_size = 1)\n",
    "\n",
    "    v_loss = 1_000_000\n",
    "    no_change_counter = 1\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        model.train()\n",
    "        loop = tqdm(training_generator, disable = not args.verbose)\n",
    "\n",
    "        # Loop over batches in an epoch using DataLoader\n",
    "        for _, data in enumerate(loop):\n",
    "            inputs = data[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(**inputs).logits\n",
    "            loss = loss_f(logits , data[1].to(device))\n",
    "            loss.backward()       \n",
    "            optimizer.step()\n",
    "            last_loss = loss.item()\n",
    "        \n",
    "        v_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for _, data in enumerate(validation_generator):\n",
    "                inputs = data[0].to(device)\n",
    "                logits = model(**inputs).logits\n",
    "                loss = loss_f(logits , data[1].to(device))\n",
    "                v_losses.append(loss)\n",
    "            \n",
    "            v_loss_epoch = sum(v_losses) / len(v_losses)\n",
    "            utils.write_log(f'lr {lr:8f} train loss {last_loss:.8f} val loss {v_loss_epoch:.8f}')\n",
    "\n",
    "            if v_loss_epoch < v_loss:\n",
    "                v_loss = v_loss_epoch\n",
    "                no_change_counter = 0\n",
    "                torch.save(model.state_dict(), utils.get_timestamp()+'chkpnt.pt')\n",
    "            elif no_change_counter > args.patience - 1:\n",
    "                break\n",
    "            else:\n",
    "                no_change_counter += 1\n",
    "        \n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args,\n",
    "             relations,\n",
    "             training_triples,\n",
    "             validation_triples,\n",
    "             testing_triples,\n",
    "             utils,\n",
    "             generator,\n",
    "             device,\n",
    "             ) -> None:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def load_evaluation_triples(triples, data_set):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, _ in enumerate(data_set):\n",
    "            item = data_set.gettext(i)\n",
    "            key = item[3] + '_' + item[4]\n",
    "            rel_list = triples.setdefault(key, [])\n",
    "            rel_index = data_set.relations.index(item[1])\n",
    "            rel_list.append(rel_index)\n",
    "            triples[key] = rel_list\n",
    "        \n",
    "        return triples\n",
    "\n",
    "    # Initialize metrices\n",
    "    ranks = []\n",
    "    ranks_filtered = []\n",
    "    hits = []\n",
    "    hits_filtered = []\n",
    "    for i in range(10):\n",
    "            hits.append([])\n",
    "            hits_filtered.append([])\n",
    "    \n",
    "    # Load datasets\n",
    "    all_triples = {}\n",
    "    training_set = KGDataset(args=args,\n",
    "                             relations=relations,\n",
    "                             triples_filename=training_triples)\n",
    "    all_triples = load_evaluation_triples(all_triples, training_set)\n",
    "    del training_set\n",
    "    \n",
    "    validation_set = KGDataset(args=args,\n",
    "                               relations=relations,\n",
    "                               triples_filename=validation_triples)\n",
    "    all_triples = load_evaluation_triples(all_triples, validation_set)\n",
    "    del validation_set\n",
    "    \n",
    "    testing_set = KGDataset(args=args,\n",
    "                               relations=relations,\n",
    "                               triples_filename=testing_triples)\n",
    "    all_triples = load_evaluation_triples(all_triples, testing_set)\n",
    "    testing_generator = DataLoader(testing_set,\n",
    "                                      batch_size = args.batch_size,\n",
    "                                    worker_init_fn=utils.seed_worker,\n",
    "                                    generator=generator,)\n",
    "    \n",
    "    # Initialize Llama\n",
    "    llama = Llama(relations, args.model_id, args.repo_token, 0.0)\n",
    "    model = llama.get_model()\n",
    "    model.resize_token_embeddings(testing_set.tokenizer_len())\n",
    "    if args.checkpoint_path == '':\n",
    "        model.load_state_dict(torch.load(utils.get_timestamp()+'chkpnt.pt', ))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(args.checkpoint_path))\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(testing_generator, disable = not args.verbose)\n",
    "        for id, data in enumerate(loop):\n",
    "            inputs = data[0].to(device)\n",
    "            data[1] = data[1].to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            for i, item in enumerate(logits):\n",
    "                gold_index = torch.argmax(data[1][i])\n",
    "                indices = torch.argsort(item, descending = True)\n",
    "                rank = (indices==gold_index).nonzero().item()\n",
    "                item_id = id * args.batch_size + i\n",
    "                \n",
    "                ranks.append(rank + 1)\n",
    "\n",
    "                # filter work\n",
    "                filter_rank = rank\n",
    "\n",
    "                # get higher predicted relations\n",
    "                # indices_list = indices.view(-1)\n",
    "                indices_list = indices.tolist()\n",
    "                higher_rels = indices_list[: indices_list.index(gold_index) ]\n",
    "                \n",
    "                # get gold relations for the triple\n",
    "                triple = testing_set.gettext(item_id)\n",
    "                key = triple[3] + '_' + triple[4]\n",
    "                rel_list = all_triples[key]\n",
    "\n",
    "                # loop higher rels\n",
    "                for j, rel_id in enumerate(higher_rels):\n",
    "                    if rel_id in rel_list:\n",
    "                        filter_rank -= 1\n",
    "                \n",
    "                ranks_filtered.append(filter_rank + 1)\n",
    "\n",
    "                # Hits work\n",
    "                failure_str = ''\n",
    "                for hits_level in range(10):\n",
    "                    if rank <= hits_level:\n",
    "                        hits[hits_level].append(1.0)\n",
    "                    else:\n",
    "                        hits[hits_level].append(0.0)\n",
    "                    \n",
    "                    if filter_rank <= hits_level:\n",
    "                        hits_filtered[hits_level].append(1.0)\n",
    "                    else:\n",
    "                        hits_filtered[hits_level].append(0.0)\n",
    "                    \n",
    "                    if rank > 10 and hits_level == 9:\n",
    "                        h, r, t, e1, e2 = testing_set.gettext(item_id)\n",
    "                        failure_str += str(rank) + '^' +str(item_id) + '^' + h + '^' + r + '^' + t + '^' + e1 + '^' + e2\n",
    "                \n",
    "                utils.write_log(failure_str,'failures.out', False)\n",
    "\n",
    "    utils.write_log(f'\\n{\"MR\":<15} {np.mean(ranks):.4f}')\n",
    "    utils.write_log(f'{\"MR Filtered\":<15} {np.mean(ranks_filtered):.4f}')\n",
    "    for i in [0,4,9]:\n",
    "        utils.write_log(f'Raw Hits           {i + 1:<3}: {np.mean(hits[i]):<5.6f}')\n",
    "        utils.write_log(f'Raw Filtered Hits  {i + 1:<3}: {np.mean(hits_filtered[i]):<5.6f}')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_size', required = True, type = float)\n",
    "    parser.add_argument('--dataset_directory', required = True, type = str)\n",
    "    parser.add_argument('--entities_filename', required = True, type = str)\n",
    "    parser.add_argument('--model_id', required = True, type = str)\n",
    "    parser.add_argument('--repo_token', required = True, type = str)\n",
    "    parser.add_argument('--padding', required = True, type = int)\n",
    "    parser.add_argument('--learning_rate', required = True, type = float)\n",
    "    parser.add_argument('--decay', required = True, type = float)\n",
    "    parser.add_argument('--task', required = True, type = str)\n",
    "    parser.add_argument('--batch_size', required = True, type = int)\n",
    "    parser.add_argument('--patience', required = True, type = int)\n",
    "    parser.add_argument('--epochs', required = True, type = int)\n",
    "    parser.add_argument('--verbose', required = True)\n",
    "    parser.add_argument('--attention_dropout', required = True, type=float)\n",
    "    parser.add_argument('--checkpoint_path', required = True, type=str)\n",
    "    \n",
    "    \n",
    "    # Keep for online repo\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # Random seeds\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(0)\n",
    "\n",
    "    # Initializations\n",
    "    \n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')+'.out'\n",
    "\n",
    "    # To be removed from the online repo\n",
    "\n",
    "    # Load utilities\n",
    "    utils = Utils(time_stamp)\n",
    "    relations = utils.load_relations(args.dataset_directory + '/relations.txt')    \n",
    "\n",
    "    # Loading GPU\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    utils.write_log('\\ndevice ' + str(device))\n",
    "\n",
    "    if args.task == 'train':\n",
    "        train(relations=relations, args=args, \\\n",
    "              utils = utils, generator=g,\n",
    "              training_triples='train.tsv',\n",
    "              validation_triples='dev.tsv',\n",
    "              device=device)\n",
    "    else:\n",
    "        evaluate(relations=relations, args=args, \\\n",
    "              utils = utils, generator=g,\n",
    "              training_triples='train.tsv',\n",
    "              validation_triples='dev.tsv',\n",
    "              testing_triples='test.tsv',\n",
    "              device=device)\n",
    "    \n",
    "    # To be removed from online repo, used in tuning\n",
    "    if False:\n",
    "        evaluate(relations=relations, args=args, \\\n",
    "              utils = utils, generator=g,\n",
    "              training_triples='train.tsv',\n",
    "              validation_triples='dev.tsv',\n",
    "              testing_triples='test.tsv',\n",
    "              device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be removed from online repo\n",
    "class Args(argparse.Namespace):\n",
    "    pass\n",
    "\n",
    "args=Args()\n",
    "args.model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "args.data_size = 0.005\n",
    "args.dataset_directory = 'data/FB15K'\n",
    "args.entities_filename = 'entity2text.txt'\n",
    "args.padding = 50\n",
    "args.batch_size = 32\n",
    "args.patience = 5\n",
    "args.repo_token = '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'\n",
    "args.task = 'train'\n",
    "args.checkpoint_path = 'train'\n",
    "args.learning_rate = 5e-5\n",
    "args.decay = 0.25\n",
    "args.epochs = 3\n",
    "args.verbose = True\n",
    "args.attention_dropout = 0.1\n",
    "args.checkpoint_path = ''\n",
    "\n",
    "# Train\n",
    "if __name__ == \"__main__\": main()\n",
    "\n",
    "# Tune\n",
    "if False:\n",
    "    for lr in [1e-04, 5e-05, 1e-05]:\n",
    "        for dc in [0.5, 0.25]:\n",
    "            for att in [0.25, 0.1, 0.0]:\n",
    "                args.attention_dropout = att\n",
    "                args.learning_rate = lr\n",
    "                args.decay = dc\n",
    "                args.verbose = False\n",
    "                main()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
